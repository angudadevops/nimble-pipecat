{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "1db60caf-a890-4e62-8255-62fd691cd6e6",
   "metadata": {},
   "source": [
    "# Deploy a Voice AI bot with Pipecat AI and NIM (and Riva TTS & STT)\n",
    "In this notebook, we walk through how to craft and deploy a voice AI bot using Pipecat AI. We illustrate the basic Pipecat flow with the `nvidia/llama-3.1-nemotron-70b-instruct` LLM model and Riva for STT (Speech-To-Text) & TTS (Text-To-Speech). However, Pipecat is not opinionated and other models and STT/TTS services can easily be used. See [Pipecat documentation](https://docs.pipecat.ai/server/services/supported-services#supported-services) for other supported services.\n",
    "\n",
    "Pipecat AI is an open-source framework for building voice and multimodal conversational agents. Pipecat simplifies the complex voice-to-voice AI pipeline, and lets developers build AI capabilities easily and with Open Source, commercial, and custom models. See [Pipecat's Core Concepts](https://docs.pipecat.ai/getting-started/core-concepts) for a deep dive into how it works.\n",
    "\n",
    "The framework was developed by Daily, a company that has provided real-time video and audio communication infrastructure since 2016. It is fully vendor neutral and is not tightly coupled to Daily's infrastructure. That said, we do use it in this demo. Sign up for a Daily-bot API key [here](https://bots.daily.co/sign-up)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b4fa7d7-88fb-4b33-8145-ee1a91e58af1",
   "metadata": {},
   "source": [
    "## Step 1 - Install dependencies\n",
    "First we set our environment.\n",
    "\n",
    "We use Daily for transport, OpenAI for context aggregation, Riva for TTS & TTS, and Silero for VAD (Voice Activity Detection). If using different services, for example Cartesia for TTS, one would run `pip install pipecat-ai[cartesia]`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "718d7f76-bb78-4614-ab77-229ed3eea402",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: python-dotenv in ./venv/lib/python3.12/site-packages (1.0.1)\n",
      "\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m24.2\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m24.3.1\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpip install --upgrade pip\u001b[0m\n",
      "Requirement already satisfied: pipecat-ai[daily,openai,riva,silero] in ./venv/lib/python3.12/site-packages (0.0.50)\n",
      "Requirement already satisfied: aiohttp~=3.11.9 in ./venv/lib/python3.12/site-packages (from pipecat-ai[daily,openai,riva,silero]) (3.11.10)\n",
      "Requirement already satisfied: loguru~=0.7.2 in ./venv/lib/python3.12/site-packages (from pipecat-ai[daily,openai,riva,silero]) (0.7.3)\n",
      "Requirement already satisfied: Markdown~=3.7 in ./venv/lib/python3.12/site-packages (from pipecat-ai[daily,openai,riva,silero]) (3.7)\n",
      "Requirement already satisfied: numpy~=1.26.4 in ./venv/lib/python3.12/site-packages (from pipecat-ai[daily,openai,riva,silero]) (1.26.4)\n",
      "Requirement already satisfied: Pillow~=10.4.0 in ./venv/lib/python3.12/site-packages (from pipecat-ai[daily,openai,riva,silero]) (10.4.0)\n",
      "Requirement already satisfied: protobuf~=5.29.1 in ./venv/lib/python3.12/site-packages (from pipecat-ai[daily,openai,riva,silero]) (5.29.1)\n",
      "Requirement already satisfied: pydantic~=2.8.2 in ./venv/lib/python3.12/site-packages (from pipecat-ai[daily,openai,riva,silero]) (2.8.2)\n",
      "Requirement already satisfied: pyloudnorm~=0.1.1 in ./venv/lib/python3.12/site-packages (from pipecat-ai[daily,openai,riva,silero]) (0.1.1)\n",
      "Requirement already satisfied: resampy~=0.4.3 in ./venv/lib/python3.12/site-packages (from pipecat-ai[daily,openai,riva,silero]) (0.4.3)\n",
      "Requirement already satisfied: daily-python~=0.13.0 in ./venv/lib/python3.12/site-packages (from pipecat-ai[daily,openai,riva,silero]) (0.13.0)\n",
      "Requirement already satisfied: openai~=1.50.2 in ./venv/lib/python3.12/site-packages (from pipecat-ai[daily,openai,riva,silero]) (1.50.2)\n",
      "Requirement already satisfied: websockets~=13.1 in ./venv/lib/python3.12/site-packages (from pipecat-ai[daily,openai,riva,silero]) (13.1)\n",
      "Requirement already satisfied: python-deepcompare~=1.0.1 in ./venv/lib/python3.12/site-packages (from pipecat-ai[daily,openai,riva,silero]) (1.0.1)\n",
      "Requirement already satisfied: nvidia-riva-client~=2.17.0 in ./venv/lib/python3.12/site-packages (from pipecat-ai[daily,openai,riva,silero]) (2.17.0)\n",
      "Requirement already satisfied: onnxruntime~=1.20.1 in ./venv/lib/python3.12/site-packages (from pipecat-ai[daily,openai,riva,silero]) (1.20.1)\n",
      "Requirement already satisfied: aiohappyeyeballs>=2.3.0 in ./venv/lib/python3.12/site-packages (from aiohttp~=3.11.9->pipecat-ai[daily,openai,riva,silero]) (2.4.4)\n",
      "Requirement already satisfied: aiosignal>=1.1.2 in ./venv/lib/python3.12/site-packages (from aiohttp~=3.11.9->pipecat-ai[daily,openai,riva,silero]) (1.3.1)\n",
      "Requirement already satisfied: attrs>=17.3.0 in ./venv/lib/python3.12/site-packages (from aiohttp~=3.11.9->pipecat-ai[daily,openai,riva,silero]) (24.2.0)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in ./venv/lib/python3.12/site-packages (from aiohttp~=3.11.9->pipecat-ai[daily,openai,riva,silero]) (1.5.0)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in ./venv/lib/python3.12/site-packages (from aiohttp~=3.11.9->pipecat-ai[daily,openai,riva,silero]) (6.1.0)\n",
      "Requirement already satisfied: propcache>=0.2.0 in ./venv/lib/python3.12/site-packages (from aiohttp~=3.11.9->pipecat-ai[daily,openai,riva,silero]) (0.2.1)\n",
      "Requirement already satisfied: yarl<2.0,>=1.17.0 in ./venv/lib/python3.12/site-packages (from aiohttp~=3.11.9->pipecat-ai[daily,openai,riva,silero]) (1.18.3)\n",
      "Requirement already satisfied: setuptools==70.0.0 in ./venv/lib/python3.12/site-packages (from nvidia-riva-client~=2.17.0->pipecat-ai[daily,openai,riva,silero]) (70.0.0)\n",
      "Requirement already satisfied: grpcio==1.65.4 in ./venv/lib/python3.12/site-packages (from nvidia-riva-client~=2.17.0->pipecat-ai[daily,openai,riva,silero]) (1.65.4)\n",
      "Requirement already satisfied: grpcio-tools==1.65.4 in ./venv/lib/python3.12/site-packages (from nvidia-riva-client~=2.17.0->pipecat-ai[daily,openai,riva,silero]) (1.65.4)\n",
      "Requirement already satisfied: coloredlogs in ./venv/lib/python3.12/site-packages (from onnxruntime~=1.20.1->pipecat-ai[daily,openai,riva,silero]) (15.0.1)\n",
      "Requirement already satisfied: flatbuffers in ./venv/lib/python3.12/site-packages (from onnxruntime~=1.20.1->pipecat-ai[daily,openai,riva,silero]) (24.3.25)\n",
      "Requirement already satisfied: packaging in ./venv/lib/python3.12/site-packages (from onnxruntime~=1.20.1->pipecat-ai[daily,openai,riva,silero]) (24.2)\n",
      "Requirement already satisfied: sympy in ./venv/lib/python3.12/site-packages (from onnxruntime~=1.20.1->pipecat-ai[daily,openai,riva,silero]) (1.13.3)\n",
      "Requirement already satisfied: anyio<5,>=3.5.0 in ./venv/lib/python3.12/site-packages (from openai~=1.50.2->pipecat-ai[daily,openai,riva,silero]) (4.7.0)\n",
      "Requirement already satisfied: distro<2,>=1.7.0 in ./venv/lib/python3.12/site-packages (from openai~=1.50.2->pipecat-ai[daily,openai,riva,silero]) (1.9.0)\n",
      "Requirement already satisfied: httpx<1,>=0.23.0 in ./venv/lib/python3.12/site-packages (from openai~=1.50.2->pipecat-ai[daily,openai,riva,silero]) (0.28.1)\n",
      "Requirement already satisfied: jiter<1,>=0.4.0 in ./venv/lib/python3.12/site-packages (from openai~=1.50.2->pipecat-ai[daily,openai,riva,silero]) (0.8.2)\n",
      "Requirement already satisfied: sniffio in ./venv/lib/python3.12/site-packages (from openai~=1.50.2->pipecat-ai[daily,openai,riva,silero]) (1.3.1)\n",
      "Requirement already satisfied: tqdm>4 in ./venv/lib/python3.12/site-packages (from openai~=1.50.2->pipecat-ai[daily,openai,riva,silero]) (4.67.1)\n",
      "Requirement already satisfied: typing-extensions<5,>=4.11 in ./venv/lib/python3.12/site-packages (from openai~=1.50.2->pipecat-ai[daily,openai,riva,silero]) (4.12.2)\n",
      "Requirement already satisfied: annotated-types>=0.4.0 in ./venv/lib/python3.12/site-packages (from pydantic~=2.8.2->pipecat-ai[daily,openai,riva,silero]) (0.7.0)\n",
      "Requirement already satisfied: pydantic-core==2.20.1 in ./venv/lib/python3.12/site-packages (from pydantic~=2.8.2->pipecat-ai[daily,openai,riva,silero]) (2.20.1)\n",
      "Requirement already satisfied: scipy>=1.0.1 in ./venv/lib/python3.12/site-packages (from pyloudnorm~=0.1.1->pipecat-ai[daily,openai,riva,silero]) (1.14.1)\n",
      "Requirement already satisfied: future>=0.16.0 in ./venv/lib/python3.12/site-packages (from pyloudnorm~=0.1.1->pipecat-ai[daily,openai,riva,silero]) (1.0.0)\n",
      "Requirement already satisfied: numba>=0.53 in ./venv/lib/python3.12/site-packages (from resampy~=0.4.3->pipecat-ai[daily,openai,riva,silero]) (0.60.0)\n",
      "Requirement already satisfied: idna>=2.8 in ./venv/lib/python3.12/site-packages (from anyio<5,>=3.5.0->openai~=1.50.2->pipecat-ai[daily,openai,riva,silero]) (3.10)\n",
      "Requirement already satisfied: certifi in ./venv/lib/python3.12/site-packages (from httpx<1,>=0.23.0->openai~=1.50.2->pipecat-ai[daily,openai,riva,silero]) (2024.8.30)\n",
      "Requirement already satisfied: httpcore==1.* in ./venv/lib/python3.12/site-packages (from httpx<1,>=0.23.0->openai~=1.50.2->pipecat-ai[daily,openai,riva,silero]) (1.0.7)\n",
      "Requirement already satisfied: h11<0.15,>=0.13 in ./venv/lib/python3.12/site-packages (from httpcore==1.*->httpx<1,>=0.23.0->openai~=1.50.2->pipecat-ai[daily,openai,riva,silero]) (0.14.0)\n",
      "Requirement already satisfied: llvmlite<0.44,>=0.43.0dev0 in ./venv/lib/python3.12/site-packages (from numba>=0.53->resampy~=0.4.3->pipecat-ai[daily,openai,riva,silero]) (0.43.0)\n",
      "Requirement already satisfied: humanfriendly>=9.1 in ./venv/lib/python3.12/site-packages (from coloredlogs->onnxruntime~=1.20.1->pipecat-ai[daily,openai,riva,silero]) (10.0)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in ./venv/lib/python3.12/site-packages (from sympy->onnxruntime~=1.20.1->pipecat-ai[daily,openai,riva,silero]) (1.3.0)\n",
      "\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m24.2\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m24.3.1\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpip install --upgrade pip\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "!pip install python-dotenv\n",
    "%load_ext dotenv\n",
    "%dotenv\n",
    "\n",
    "!pip install \"pipecat-ai[daily,openai,riva,silero]\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7979c5d1-97a9-42e7-9de2-88b7d31b1409",
   "metadata": {},
   "source": [
    "## Step 2 - Configure Daily transport for WebRTC communication\n",
    "- room_url: Where to connect (and where will navigate to to talk to our bot)\n",
    "- None: No authentication token needed\n",
    "- \"NVIDIA NIM\": The bot's display name\n",
    "- Enable audio output for text-to-speech playback and enable VAD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "5efb09c6-b9f7-409a-9ad4-742f494f6367",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Url to talk to the NVIDIA NIM bot\n",
    "# Update to your room url after obtaining Daily-bot API key\n",
    "#### NOTE: if this is changed, the link in Step 7 (HERE) will no longer work.\n",
    "DAILY_SAMPLE_ROOM_URL=\"https://pc-34b1bdc94a7741719b57b2efb82d658e.daily.co/prod-test\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "bfe1be9b-e052-4430-b7e1-d7bf57a5ad9e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m2024-12-12 22:14:15.138\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mpipecat.audio.vad.vad_analyzer\u001b[0m:\u001b[36mset_params\u001b[0m:\u001b[36m69\u001b[0m - \u001b[1mSetting VAD params to: confidence=0.7 start_secs=0.2 stop_secs=0.8 min_volume=0.6\u001b[0m\n",
      "\u001b[32m2024-12-12 22:14:15.139\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36mpipecat.audio.vad.silero\u001b[0m:\u001b[36m__init__\u001b[0m:\u001b[36m114\u001b[0m - \u001b[34m\u001b[1mLoading Silero VAD model...\u001b[0m\n",
      "\u001b[32m2024-12-12 22:14:15.237\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36mpipecat.audio.vad.silero\u001b[0m:\u001b[36m__init__\u001b[0m:\u001b[36m136\u001b[0m - \u001b[34m\u001b[1mLoaded Silero VAD\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Participant left: {'id': '8613db22-b75e-4494-8259-232a6fd74f00', 'info': {'isOwner': False, 'joinedAt': 1734063286, 'permissions': {'hasPresence': True, 'canAdmin': [], 'canSend': ['customVideo', 'camera', 'customAudio', 'microphone', 'screenAudio', 'screenVideo']}, 'userName': 'vanessa', 'isLocal': False}}\n"
     ]
    }
   ],
   "source": [
    "from pipecat.audio.vad.silero import SileroVADAnalyzer\n",
    "from pipecat.transports.services.daily import DailyParams, DailyTransport\n",
    "\n",
    "transport = DailyTransport(\n",
    "    DAILY_SAMPLE_ROOM_URL,\n",
    "    None,\n",
    "    \"NVIDIA NIM\",\n",
    "    DailyParams(\n",
    "        audio_out_enabled=True,\n",
    "        vad_enabled=True,\n",
    "        vad_analyzer=SileroVADAnalyzer(),\n",
    "        vad_audio_passthrough=True,\n",
    "    ),\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8506527e-b84c-49e1-8af4-223fdb33f582",
   "metadata": {},
   "source": [
    "## Step 3 - Initialize LLM, STT, and TTS services\n",
    "We can customize options, for example a different LLM `model` or `voice_id` for FastPitch TTS."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "623d77d5-c183-43d0-980d-fd99a2836365",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m2024-12-12 22:14:46.247\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36mpipecat.services.openai\u001b[0m:\u001b[36m_stream_chat_completions\u001b[0m:\u001b[36m174\u001b[0m - \u001b[34m\u001b[1mGenerating chat: [{\"role\": \"system\", \"content\": \"You are a helpful LLM in a WebRTC call. Your goal is to demonstrate your capabilities in a succinct way. Your output will be converted to audio so don't include special characters in your answers. Respond to what the user said in a creative and helpful way that makes a cat pun if it is possible.\", \"name\": \"system\"}, {\"role\": \"system\", \"content\": \"Please introduce yourself to the user and deliver a cat fact.\", \"name\": \"system\"}]\u001b[0m\n",
      "\u001b[32m2024-12-12 22:14:46.795\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36mpipecat.services.riva\u001b[0m:\u001b[36mrun_tts\u001b[0m:\u001b[36m98\u001b[0m - \u001b[34m\u001b[1mGenerating TTS: [Hello there, it's great to connect with you.]\u001b[0m\n",
      "\u001b[32m2024-12-12 22:14:47.824\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36mpipecat.services.riva\u001b[0m:\u001b[36mrun_tts\u001b[0m:\u001b[36m98\u001b[0m - \u001b[34m\u001b[1mGenerating TTS: [ I'm your friendly AI assistant, here to help you navigate the call and provide some fun facts along the way.]\u001b[0m\n",
      "\u001b[32m2024-12-12 22:14:48.536\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36mpipecat.services.riva\u001b[0m:\u001b[36mrun_tts\u001b[0m:\u001b[36m98\u001b[0m - \u001b[34m\u001b[1mGenerating TTS: [ To get us started, did you know that cats have scent glands on their faces?]\u001b[0m\n",
      "\u001b[32m2024-12-12 22:14:48.876\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36mpipecat.services.riva\u001b[0m:\u001b[36mrun_tts\u001b[0m:\u001b[36m98\u001b[0m - \u001b[34m\u001b[1mGenerating TTS: [ It's true - they have a unique organ called the vomeronasal organ, or Jacobson's organ, that helps them detect pheromones.]\u001b[0m\n",
      "\u001b[32m2024-12-12 22:14:49.318\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36mpipecat.services.riva\u001b[0m:\u001b[36mrun_tts\u001b[0m:\u001b[36m98\u001b[0m - \u001b[34m\u001b[1mGenerating TTS: [ Who knew our feline friends were such purr-fectly informed creatures?]\u001b[0m\n",
      "\u001b[32m2024-12-12 22:14:49.624\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36mpipecat.services.riva\u001b[0m:\u001b[36mrun_tts\u001b[0m:\u001b[36m98\u001b[0m - \u001b[34m\u001b[1mGenerating TTS: [ What's on your mind today?]\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "from pipecat.services.nim import NimLLMService\n",
    "from pipecat.services.riva import FastPitchTTSService, ParakeetSTTService\n",
    "\n",
    "stt = ParakeetSTTService(api_key=os.getenv(\"NVIDIA_API_KEY\"))\n",
    "\n",
    "llm = NimLLMService(\n",
    "    api_key=os.getenv(\"NVIDIA_API_KEY\"), model=\"meta/llama-3.1-70b-instruct\"\n",
    ")\n",
    "\n",
    "tts = FastPitchTTSService(api_key=os.getenv(\"NVIDIA_API_KEY\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ac150732-cbb4-4c70-8d31-cab5ae51b5fb",
   "metadata": {},
   "source": [
    "## Step 4 - Define prompt and initialize context aggregator\n",
    "Edit the prompt as desired."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "0d884775-c4c0-49eb-b502-d4c855cc8e3b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pipecat.processors.aggregators.openai_llm_context import OpenAILLMContext\n",
    "\n",
    "messages = [\n",
    "    {\n",
    "        \"role\": \"system\",\n",
    "        \"content\": \"You are a helpful LLM in a WebRTC call. Your goal is to demonstrate your capabilities in a succinct way. Your output will be converted to audio so don't include special characters in your answers. Respond to what the user said in a creative and helpful way that makes a cat pun if it is possible.\",\n",
    "    },\n",
    "]\n",
    "\n",
    "context = OpenAILLMContext(messages)\n",
    "context_aggregator = llm.create_context_aggregator(context)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0752c614-a65d-4c61-965f-26d7b46f8153",
   "metadata": {},
   "source": [
    "## Step 5 - Create pipeline\n",
    "Here we align the services into a pipeline to process speech into text, send to llm, then turn the llm response text into speech."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "7f8620a2-4caa-40c5-88d9-8aca2743157e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m2024-12-12 22:14:23.001\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36mpipecat.processors.frame_processor\u001b[0m:\u001b[36mlink\u001b[0m:\u001b[36m150\u001b[0m - \u001b[34m\u001b[1mLinking PipelineSource#0 -> DailyInputTransport#0\u001b[0m\n",
      "\u001b[32m2024-12-12 22:14:23.001\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36mpipecat.processors.frame_processor\u001b[0m:\u001b[36mlink\u001b[0m:\u001b[36m150\u001b[0m - \u001b[34m\u001b[1mLinking DailyInputTransport#0 -> ParakeetSTTService#0\u001b[0m\n",
      "\u001b[32m2024-12-12 22:14:23.002\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36mpipecat.processors.frame_processor\u001b[0m:\u001b[36mlink\u001b[0m:\u001b[36m150\u001b[0m - \u001b[34m\u001b[1mLinking ParakeetSTTService#0 -> OpenAIUserContextAggregator#0\u001b[0m\n",
      "\u001b[32m2024-12-12 22:14:23.003\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36mpipecat.processors.frame_processor\u001b[0m:\u001b[36mlink\u001b[0m:\u001b[36m150\u001b[0m - \u001b[34m\u001b[1mLinking OpenAIUserContextAggregator#0 -> NimLLMService#0\u001b[0m\n",
      "\u001b[32m2024-12-12 22:14:23.003\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36mpipecat.processors.frame_processor\u001b[0m:\u001b[36mlink\u001b[0m:\u001b[36m150\u001b[0m - \u001b[34m\u001b[1mLinking NimLLMService#0 -> FastPitchTTSService#0\u001b[0m\n",
      "\u001b[32m2024-12-12 22:14:23.004\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36mpipecat.processors.frame_processor\u001b[0m:\u001b[36mlink\u001b[0m:\u001b[36m150\u001b[0m - \u001b[34m\u001b[1mLinking FastPitchTTSService#0 -> DailyOutputTransport#0\u001b[0m\n",
      "\u001b[32m2024-12-12 22:14:23.005\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36mpipecat.processors.frame_processor\u001b[0m:\u001b[36mlink\u001b[0m:\u001b[36m150\u001b[0m - \u001b[34m\u001b[1mLinking DailyOutputTransport#0 -> OpenAIAssistantContextAggregator#0\u001b[0m\n",
      "\u001b[32m2024-12-12 22:14:23.005\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36mpipecat.processors.frame_processor\u001b[0m:\u001b[36mlink\u001b[0m:\u001b[36m150\u001b[0m - \u001b[34m\u001b[1mLinking OpenAIAssistantContextAggregator#0 -> PipelineSink#0\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "from pipecat.pipeline.pipeline import Pipeline\n",
    "\n",
    "pipeline = Pipeline(\n",
    "    [\n",
    "        transport.input(),  # Transport user input\n",
    "        stt,  # STT\n",
    "        context_aggregator.user(),  # User responses\n",
    "        llm,  # LLM\n",
    "        tts,  # TTS\n",
    "        transport.output(),  # Transport bot output\n",
    "        context_aggregator.assistant(),  # Assistant spoken responses\n",
    "    ]\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ad9c588f-0c00-4414-984a-33da31e2803d",
   "metadata": {},
   "source": [
    "## Step 6 - Create PipelineTask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "9fbadb9a-9778-4f0f-910f-5c53d117e593",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m2024-12-12 22:14:26.329\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36mpipecat.processors.frame_processor\u001b[0m:\u001b[36mlink\u001b[0m:\u001b[36m150\u001b[0m - \u001b[34m\u001b[1mLinking Source#0 -> Pipeline#0\u001b[0m\n",
      "\u001b[32m2024-12-12 22:14:26.330\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36mpipecat.processors.frame_processor\u001b[0m:\u001b[36mlink\u001b[0m:\u001b[36m150\u001b[0m - \u001b[34m\u001b[1mLinking Pipeline#0 -> Sink#0\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "from pipecat.pipeline.task import PipelineParams, PipelineTask\n",
    "\n",
    "task = PipelineTask(pipeline, PipelineParams(allow_interruptions=True))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b4890ce7-6a1a-4f39-b6af-9a3335ad9fcf",
   "metadata": {},
   "source": [
    "## Step 7 - Create a pipeline runner\n",
    "This manages the processing pipeline."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "87e504ab-b889-4b6a-96a1-159d42a95833",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pipecat.pipeline.runner import PipelineRunner\n",
    "\n",
    "runner = PipelineRunner()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c162c265-39cd-49d1-beb6-fcf368572156",
   "metadata": {},
   "source": [
    "## Step 8 - Set event handlers\n",
    "The `on_first_participant_joined` handler tells the bot to start the conversation when you join the call.  \n",
    "The `on_participant_left` handler sends an EndFrame which signals to terminate the pipeline."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "a2917234-efc6-440d-b427-ca4acab0b194",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pipecat.frames.frames import LLMMessagesFrame, EndFrame\n",
    "\n",
    "@transport.event_handler(\"on_first_participant_joined\")\n",
    "async def on_first_participant_joined(transport, participant):\n",
    "    # Kick off the conversation.\n",
    "    messages.append({\"role\": \"system\", \"content\": \"Please introduce yourself to the user and deliver a cat fact.\"})\n",
    "    await task.queue_frames([LLMMessagesFrame(messages)])\n",
    "\n",
    "@transport.event_handler(\"on_participant_left\")\n",
    "async def on_participant_left(transport, participant, reason):\n",
    "    print(f\"Participant left: {participant}\")\n",
    "    await task.queue_frame(EndFrame())   "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "08998f8d-ac33-4b38-b10a-01691f81636a",
   "metadata": {},
   "source": [
    "## Step 7 - Run the bot! Then talk to the bot [HERE](https://pc-34b1bdc94a7741719b57b2efb82d658e.daily.co/prod-test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "92a411cb-d2c8-4446-be69-b391486e853e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m2024-12-12 22:14:34.062\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36mpipecat.pipeline.runner\u001b[0m:\u001b[36mrun\u001b[0m:\u001b[36m27\u001b[0m - \u001b[34m\u001b[1mRunner PipelineRunner#0 started running PipelineTask#0\u001b[0m\n",
      "\u001b[32m2024-12-12 22:14:34.063\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mpipecat.transports.services.daily\u001b[0m:\u001b[36mjoin\u001b[0m:\u001b[36m322\u001b[0m - \u001b[1mJoining https://pc-34b1bdc94a7741719b57b2efb82d658e.daily.co/prod-test\u001b[0m\n",
      "\u001b[32m2024-12-12 22:14:35.698\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mpipecat.transports.services.daily\u001b[0m:\u001b[36mjoin\u001b[0m:\u001b[36m340\u001b[0m - \u001b[1mJoined https://pc-34b1bdc94a7741719b57b2efb82d658e.daily.co/prod-test\u001b[0m\n",
      "\u001b[32m2024-12-12 22:14:46.244\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mpipecat.transports.services.daily\u001b[0m:\u001b[36mon_participant_joined\u001b[0m:\u001b[36m595\u001b[0m - \u001b[1mParticipant joined 8613db22-b75e-4494-8259-232a6fd74f00\u001b[0m\n",
      "\u001b[32m2024-12-12 22:14:47.823\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36mpipecat.transports.base_output\u001b[0m:\u001b[36m_bot_started_speaking\u001b[0m:\u001b[36m211\u001b[0m - \u001b[34m\u001b[1mBot started speaking\u001b[0m\n",
      "\u001b[32m2024-12-12 22:15:03.555\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36mpipecat.transports.base_input\u001b[0m:\u001b[36m_handle_interruptions\u001b[0m:\u001b[36m124\u001b[0m - \u001b[34m\u001b[1mUser started speaking\u001b[0m\n",
      "\u001b[32m2024-12-12 22:15:03.559\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36mpipecat.transports.base_output\u001b[0m:\u001b[36m_bot_stopped_speaking\u001b[0m:\u001b[36m218\u001b[0m - \u001b[34m\u001b[1mBot stopped speaking\u001b[0m\n",
      "\u001b[32m2024-12-12 22:15:08.255\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36mpipecat.transports.base_input\u001b[0m:\u001b[36m_handle_interruptions\u001b[0m:\u001b[36m131\u001b[0m - \u001b[34m\u001b[1mUser stopped speaking\u001b[0m\n",
      "\u001b[32m2024-12-12 22:15:08.886\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36mpipecat.services.openai\u001b[0m:\u001b[36m_stream_chat_completions\u001b[0m:\u001b[36m174\u001b[0m - \u001b[34m\u001b[1mGenerating chat: [{\"role\": \"system\", \"content\": \"You are a helpful LLM in a WebRTC call. Your goal is to demonstrate your capabilities in a succinct way. Your output will be converted to audio so don't include special characters in your answers. Respond to what the user said in a creative and helpful way that makes a cat pun if it is possible.\", \"name\": \"system\"}, {\"role\": \"system\", \"content\": \"Please introduce yourself to the user and deliver a cat fact.\", \"name\": \"system\"}, {\"role\": \"assistant\", \"content\": \"Hello there, it's great to connect with you.  I'm your friendly AI assistant, here to help you navigate the call and provide some fun facts along the way.  To get us started, did you know that cats have scent glands on their faces?  It's true - they have a unique organ called the vomeronasal organ, or Jacobson's organ, that helps them detect pheromones.  Who knew our feline friends were such purr-fectly informed creatures?  What's on your mind today?\"}, {\"role\": \"user\", \"content\": \"oh that's interesting  you tell me how many cat breeds there are \"}]\u001b[0m\n",
      "\u001b[32m2024-12-12 22:15:09.259\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36mpipecat.services.riva\u001b[0m:\u001b[36mrun_tts\u001b[0m:\u001b[36m98\u001b[0m - \u001b[34m\u001b[1mGenerating TTS: [Cat breeds are a fascinating topic.]\u001b[0m\n",
      "\u001b[32m2024-12-12 22:15:09.665\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36mpipecat.transports.base_output\u001b[0m:\u001b[36m_bot_started_speaking\u001b[0m:\u001b[36m211\u001b[0m - \u001b[34m\u001b[1mBot started speaking\u001b[0m\n",
      "\u001b[32m2024-12-12 22:15:09.667\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36mpipecat.services.riva\u001b[0m:\u001b[36mrun_tts\u001b[0m:\u001b[36m98\u001b[0m - \u001b[34m\u001b[1mGenerating TTS: [ According to the International Cat Association, there are currently 73 recognized cat breeds.]\u001b[0m\n",
      "\u001b[32m2024-12-12 22:15:10.082\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36mpipecat.services.riva\u001b[0m:\u001b[36mrun_tts\u001b[0m:\u001b[36m98\u001b[0m - \u001b[34m\u001b[1mGenerating TTS: [ And that number is always growing, as new breeds are developed and recognized.]\u001b[0m\n",
      "\u001b[32m2024-12-12 22:15:10.493\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36mpipecat.services.riva\u001b[0m:\u001b[36mrun_tts\u001b[0m:\u001b[36m98\u001b[0m - \u001b[34m\u001b[1mGenerating TTS: [ But don't worry, I won't make you paw through all 73 breeds at once.]\u001b[0m\n",
      "\u001b[32m2024-12-12 22:15:10.798\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36mpipecat.services.riva\u001b[0m:\u001b[36mrun_tts\u001b[0m:\u001b[36m98\u001b[0m - \u001b[34m\u001b[1mGenerating TTS: [ Would you like to know more about a specific breed, or perhaps learn about some of the most popular ones?]\u001b[0m\n",
      "\u001b[32m2024-12-12 22:15:20.335\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36mpipecat.transports.base_input\u001b[0m:\u001b[36m_handle_interruptions\u001b[0m:\u001b[36m124\u001b[0m - \u001b[34m\u001b[1mUser started speaking\u001b[0m\n",
      "\u001b[32m2024-12-12 22:15:20.338\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36mpipecat.transports.base_output\u001b[0m:\u001b[36m_bot_stopped_speaking\u001b[0m:\u001b[36m218\u001b[0m - \u001b[34m\u001b[1mBot stopped speaking\u001b[0m\n",
      "\u001b[32m2024-12-12 22:15:21.535\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36mpipecat.transports.base_input\u001b[0m:\u001b[36m_handle_interruptions\u001b[0m:\u001b[36m131\u001b[0m - \u001b[34m\u001b[1mUser stopped speaking\u001b[0m\n",
      "\u001b[32m2024-12-12 22:15:21.762\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mpipecat.transports.services.daily\u001b[0m:\u001b[36mon_participant_left\u001b[0m:\u001b[36m605\u001b[0m - \u001b[1mParticipant left 8613db22-b75e-4494-8259-232a6fd74f00\u001b[0m\n",
      "{\"timestamp\":\"2024-12-13T04:15:21.762439Z\",\"level\":\"ERROR\",\"fields\":{\"message\":\"no subscription for consumer: ConsumerId(\\\"f2c6a43e-3292-4cc3-9744-b363a2a258fb\\\")\"},\"target\":\"daily_core::call_manager::events::from_sfu::soup_consumer_closed\"}\n",
      "{\"timestamp\":\"2024-12-13T04:15:21.809835Z\",\"level\":\"ERROR\",\"fields\":{\"message\":\"Failed to close consumer: ConsumerNoLongerExists(ConsumerId(\\\"f2c6a43e-3292-4cc3-9744-b363a2a258fb\\\"))\"},\"target\":\"daily_core::call_manager::events::subscription::common\"}\n",
      "\u001b[32m2024-12-12 22:15:21.940\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mpipecat.transports.services.daily\u001b[0m:\u001b[36mleave\u001b[0m:\u001b[36m435\u001b[0m - \u001b[1mLeaving https://pc-34b1bdc94a7741719b57b2efb82d658e.daily.co/prod-test\u001b[0m\n",
      "\u001b[32m2024-12-12 22:15:21.949\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mpipecat.transports.services.daily\u001b[0m:\u001b[36mleave\u001b[0m:\u001b[36m443\u001b[0m - \u001b[1mLeft https://pc-34b1bdc94a7741719b57b2efb82d658e.daily.co/prod-test\u001b[0m\n",
      "\u001b[32m2024-12-12 22:15:21.951\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36mpipecat.pipeline.runner\u001b[0m:\u001b[36mrun\u001b[0m:\u001b[36m31\u001b[0m - \u001b[34m\u001b[1mRunner PipelineRunner#0 finished running PipelineTask#0\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "await runner.run(task)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python3.12",
   "language": "python",
   "name": "venv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
